---
title: Strategy for monitoring LLM Application with OpenTelemetry
linkTitle: LLM Observability
date: 2024-04-09
author: '[Ishan Jain](https://github.com/ishanjainn) (Grafana)'
# prettier-ignore
cSpell:ignore: associated chatbots Ishan ishan_jainn ishanjainn Jain llm timeframe
---

In the tech world, Large Language Models (LLMs), like the brains behind chatbots or code generators, are getting big. But as they grow, making sure they keep running smoothly is a big deal.

In the
following example, we'll use [Prometheus](https://prometheus.io/) and [Grafana Tempo](https://grafana.com/oss/tempo/) as the target
backend for metrics and traces generated by an Auto-instrumentation LLM monitoring library [OpenLIT](https://github.com/openlit/openlit). We will use [Grafana](https://grafana.com/oss/grafana/) as the tool to visualize the LLM monitoring data.

## Why Observability Matters for LLM Applications

Monitoring LLM applications is crucial for several reasons. For starters, it's vital to keep track of how often LLMs are being used, especially in RAG-based applications. This monitoring helps you understand the demand and usage patterns. Secondly, knowing how long it takes for LLMs to respond to requests and the nature of each interaction (prompt and response) can significantly impact user experience and operational efficiency. Since using these models incurs costs per request, tracking usage helps in managing expenses. Moreover, making too many requests in a short timeframe can lead to rate limiting, temporarily blocking further requests. By keeping a close eye on these aspects, you can not only save costs but also avoid hitting request limits, ensuring your LLM applications perform optimally.

## Prerequisites

Before we begin, make sure you have the following running in your environment:

- Prometheus
- Grafana Tempo
- Grafana

## Steps 

### Setting Up the OpenTelemetry Collector

First up, install the OpenTelemetry Collector. You can find how to do this in the installation guide [here](/docs/collector/installation/).

### Configuring the Collector

Next, you need to tell the collector where to send the data. Here's a simple configuration for sending metrics to Prometheus and traces to Grafana Tempo:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
  memory_limiter:
    # 80% of maximum memory up to 2G
    limit_mib: 1500
    # 25% of limit up to 2G
    spike_limit_mib: 512
    check_interval: 5s

exporters:
  prometheusremotewrite:
    endpoint: "YOUR_PROMETHEUS_REMOTE_WRITE_URL"
    add_metric_suffixes: false
  otlp:
    endpoint: "YOUR_TEMPO_URL"

service:
  pipelines:
    traces:
      receivers: [ otlp ]
      processors: [ memory_limiter, batch ]
      exporters: [ otlp ]
    metrics:
      receivers: [ otlp ]
      processors: [ memory_limiter, batch ]
      exporters: [ prometheusremotewrite ]
```

## Instrument your LLM Application with OpenLIT

### Install the library

To install the OpenLIT Python Library, run this command:

```shell
pip install openlit
```

Then, add these lines to your LLM application:

```python
import openlit

openlit.init(
  otlp_endpoint="YOUR_OTELCOL_URL:4318", 
)
```

You can instead pass the OpenTelemetry Collector URL via the `OTEL_EXPORTER_OTLP_ENDPOINT` also.

```python
import openlit

openlit.init()
```

```shell
export OTEL_EXPORTER_OTLP_ENDPOINT = "YOUR_OTELCOL_URL:4318"
```

## Visualize the metrics and traces

After your OpenTelemetry Collectors start sending metrics to Prometheus and traces to Grafana Tempo, follow
these steps to visualize them in Grafana. You can use any tool of your choice to visualize this data:

### Add Prometheus as a data source

1. In Grafana, navigate to **Connections** > **Data Sources**.
2. Click **Add data source** and select **Prometheus**.
3. In the settings, enter your Prometheus URL, for example,
   `http://<your_prometheus_host>`, along with any other necessary details.
4. Select **Save & Test**.

### Add Tempo as a data source

1. In Grafana, navigate to **Connections** > **Data Sources**.
2. Click **Add data source** and select **Tempo**.
3. In the settings, enter your Tempo URL, for example,
   `http://<your_tempo_host>`, along with any other necessary details.
4. Select **Save & Test**.

### Add the dashboard

To make things easy, you can use the dashboard the OpenLIT team made. Just grab the JSON from [here](https://docs.openlit.io/latest/connections/prometheus-tempo#dashboard).

This guide showed you how to use OpenTelemetry, Prometheus, Tempo, and Grafana to monitor your LLM Applications. 

If you have any questions, reach out on my GitHub [@ishanjainn](https://github.com/ishanjainn) or Twitter [@ishan_jainn](https://twitter.com/ishan_jainn).
