---
title: Observability for LLM-based applications using OpenTelemetry: An example
linkTitle: LLM Observability
date: 2024-04-09
author: '[Ishan Jain](https://github.com/ishanjainn) (Grafana)'
# prettier-ignore
cSpell:ignore: associated chatbots Ishan ishan_jainn ishanjainn Jain llm timeframe
---

As Large Language Models (LLMs) rapidly transform the technology landscape by
driving advanced chatbots and innovative code generators, their expanding role
underscores the necessity for robust observability mechanisms. Ensuring the
optimal performance and reliability of LLM-based applications is not just
beneficial but essential in today's tech-driven environment.

In the following example, we'll use [Prometheus](https://prometheus.io/) and
[Grafana Tempo](https://grafana.com/oss/tempo/) as the target backend for
metrics and traces generated by an auto-instrumentation LLM monitoring library
[OpenLIT](https://github.com/openlit/openlit). We will use
[Grafana](https://grafana.com/oss/grafana/) as the tool to visualize the LLM
monitoring data.

## Why Observability Matters for LLM Applications

Monitoring LLM applications is crucial for several reasons.

1. It's vital to keep track of how often LLMs are being used, especially in
   RAG-based applications. This monitoring helps you understand the demand and
   usage patterns.
2. Understanding how long it takes for LLMs to respond to requests and the
   nature of each interaction (prompt and response) can significantly impact
   user experience and operational efficiency. Since using these models incurs
   costs per request, tracking usage helps in managing expenses.
3. Making too many requests in a short timeframe can lead to rate limiting,
   temporarily blocking further requests.

By keeping a close eye on these aspects, you can not only save costs but also
avoid hitting request limits, ensuring your LLM applications perform optimally.

## What are the signal that you should be looking at?

Integrating Large Language Models (LLMs) into applications represents a pivotal
shift from traditional machine learning (ML) practices, particularly because
these LLMs are accessed via API calls rather than being hosted locally or run
in-house. This external integration necessitates a nuanced approach to
observability, where understanding both the sequence of events (through traces)
and the aggregated data (through metrics) becomes crucial for optimizing
performance and managing costs. Here's the critical signals to monitor:

### Traces

Tracing is vital for capturing the flow of requests through your system and
understanding the interactions with hosted LLM services. Here's what you need to
focus on:

- **Request Metadata**: This is even more critical in the context of LLMs than
  traditional ML, given the variety of parameters (like temperature and prompt
  details) that can drastically affect both the response quality and the cost.
  Specific aspects to monitor are:
  - **Temperature**: Indicates the level of creativity or randomness desired
    from the model's outputs. Varying this parameter can significantly impact
    the nature of the generated content.
  - **Model Name or Version**: Essential for tracking over time, as updates to
    the LLM might affect performance or response characteristics.
  - **Prompt Details**: The exact inputs sent to the LLM, which, unlike in-house
    ML models where inputs might be more controlled and homogeneous, can vary
    wildly and affect output complexity and cost implications.
- **Response Metadata**: Given the API-based interaction with LLMs, tracking the
  specifics of the response is key for cost management and quality assessment:
  - **Tokens**: Directly impacts cost and is a measure of response length and
    complexity.
  - **Cost**: Critical for budgeting, as API-based costs can scale with the
    number of requests and the complexity of each request.
  - **Response Details**: Similar to the prompt details but from the response
    perspective, providing insights into the model's output characteristics and
    potential areas of inefficiency or unexpected cost.

### Metrics

Metrics offer a quantifiable way to track and visualize how your application's
LLM integration is performing over time plus they enable a quicker overall view
to performance as compared to Traces. The key metrics to monitor include:

- **Request Volume**: The total number of requests made to the LLM service. This
  helps in understanding the demand patterns and identifying any anomaly in
  usage, such as sudden spikes or drops.
- **Request Duration**: The time it takes for a request to be processed and a
  response to be received from the LLM. This includes network latency and the
  time the LLM takes to generate a response, providing insights into the
  performance and reliability of the LLM service.
- **Costs and Tokens Counters**: Keeping track of the total cost accrued and
  tokens consumed over time is essential for budgeting and cost optimization
  strategies. Monitoring these metrics can alert you to unexpected increases
  that may indicate inefficient use of the LLM or the need for optimization.

## Prerequisites

Before we begin, make sure you have the following running in your environment:

- Prometheus
- Grafana Tempo
- Grafana

## Steps

## Setting Up the OpenTelemetry Collector

First up, install the OpenTelemetry Collector. You can find how to do this in
the installation guide [here](/docs/collector/installation/).

## Configuring the Collector

Next, you need to tell the Collector where to send the data. Here's a simple
configuration for sending metrics to Prometheus and traces to Grafana Tempo:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
  memory_limiter:
    # 80% of maximum memory up to 2G
    limit_mib: 1500
    # 25% of limit up to 2G
    spike_limit_mib: 512
    check_interval: 5s

exporters:
  prometheusremotewrite:
    endpoint: 'YOUR_PROMETHEUS_REMOTE_WRITE_URL'
    add_metric_suffixes: false
  otlp:
    endpoint: 'YOUR_TEMPO_URL'

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [prometheusremotewrite]
```

## Instrument your LLM Application with OpenLIT

OpenLIT is an OpenTelemetry-based library designed to streamline the monitoring
of LLM-based applications by offering auto-instrumentation for a variety of
Large Language Models and VectorDBs. It aligns with the GenAI semantic
conventions established by the OpenTelemetry community and ensures a smooth
integration process by not relying on vendor-specific span attributes or
environment variables for OTLP endpoint configuration, offering a universally
compatible solution.

### Install the library

To install the OpenLIT Python Library, run this command:

```shell
pip install openlit
```

Then, add these lines to your LLM application:

```python
import openlit

openlit.init(
  otlp_endpoint="YOUR_OTELCOL_URL:4318",
)
```

You can instead pass the OpenTelemetry Collector URL using the
`OTEL_EXPORTER_OTLP_ENDPOINT` also.

```python
import openlit

openlit.init()
```

```shell
export OTEL_EXPORTER_OTLP_ENDPOINT = "YOUR_OTELCOL_URL:4318"
```

## Visualize the metrics and traces

After your OpenTelemetry Collectors start sending metrics to Prometheus and
traces to Grafana Tempo, follow these steps to visualize them in Grafana. You
can use any tool of your choice to visualize this data:

### Add Prometheus as a data source

1. In Grafana, navigate to **Connections** > **Data Sources**.
2. Click **Add data source** and select **Prometheus**.
3. In the settings, enter your Prometheus URL, for example,
   `http://<your_prometheus_host>`, along with any other necessary details.
4. Select **Save & Test**.

### Add Tempo as a data source

1. In Grafana, navigate to **Connections** > **Data Sources**.
2. Click **Add data source** and select **Tempo**.
3. In the settings, enter your Tempo URL, for example,
   `http://<your_tempo_host>`, along with any other necessary details.
4. Select **Save & Test**.

### Add the dashboard

To make things easy, you can use the dashboard the OpenLIT team made. Just grab
the JSON from
[here](https://docs.openlit.io/latest/connections/prometheus-tempo#dashboard).

This guide showed you how to use OpenTelemetry, Prometheus, Tempo, and Grafana
to monitor your LLM Applications.

If you have any questions, reach out on my GitHub
[@ishanjainn](https://github.com/ishanjainn) or Twitter
[@ishan_jainn](https://twitter.com/ishan_jainn).
